{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yLZ0jE2yXV7i"
   },
   "outputs": [],
   "source": [
    "model_api_gateway = (\n",
    "    \"https://r6cl9xl3q9.execute-api.us-east-1.amazonaws.com/default/llama2API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIkG6RakXhb8",
    "outputId": "2b7c9179-9806-49a2-a53c-de299300f8ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generation': ' to be happy and to be with those you love. I think the meaning of life is to be happy and to be with those you love. I believe the meaning of life is to be happy and to be with those you love. I think the meaning of life is to be happy and to be with those you love'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "json_body = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",  ### YOUR CODE HERE,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },  ### YOUR CODE HERE }\n",
    "}\n",
    "\n",
    "response = requests.post(model_api_gateway, json=json_body)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to be happy and to be happy, you need to be healthy.\n",
      "I’m a big fan of a healthy lifestyle. I don’t think it’s something that you can do overnight. It takes time and effort. But I do believe that it’s something that you can\n"
     ]
    }
   ],
   "source": [
    "print(response.json()[0][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bWy12euzgf-r"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "K0dSlPfngh4o"
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "\n",
    "\n",
    "class Llama2SageMaker(LLM):\n",
    "    max_new_tokens: int = 256  ### YOUR CODE HERE\n",
    "    top_p: float = 0.9  ### YOUR CODE HERE\n",
    "    temperature: float = 0.6  ### YOUR CODE HERE\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        ### YOUR CODE HERE\n",
    "        return \"Llama2SageMaker\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        json_body = {\n",
    "            \"inputs\": prompt,  ### YOUR CODE HERE,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": self.max_new_tokens,\n",
    "                \"top_p\": self.top_p,\n",
    "                \"temperature\": self.temperature,\n",
    "            },  ### YOUR CODE HERE\n",
    "        }\n",
    "        while True:\n",
    "            print(\"calling llama2 API...\")\n",
    "            response = requests.post(model_api_gateway, json=json_body, timeout=None)\n",
    "            print(\"llama2 API call complete\")\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "        print(response.json())\n",
    "        return response.json()[0][\"generation\"]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EygPRQj9jG-D"
   },
   "outputs": [],
   "source": [
    "llm = Llama2SageMaker() ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "5FRbcRKgjI76",
    "outputId": "3994f622-dc3f-421c-af50-8664ddad3f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling llama2 API...\n",
      "llama2 API call complete\n",
      "[{'generation': '\\nThe answer is: 15 pounds.\\nHow much wood could a woodchuck chuck if a wood chuck could chuck wood? The answer is 15 pounds.\\nThe woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodch'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe answer is: 15 pounds.\\nHow much wood could a woodchuck chuck if a wood chuck could chuck wood? The answer is 15 pounds.\\nThe woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood.\\nThe woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodchuck can chuck 15 pounds of wood. The woodch'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"How much wood could a woodchuck chuck if a wood chuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6AjBDFCYlwmA"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IyrFqDRnmWEZ"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"https://arxiv.org/pdf/2005.11401v4.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UOdpBdXxn7qO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragas 0.0.11 requires protobuf<=3.20.0, but you have protobuf 4.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U openai chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgMOeJSwnr4D",
    "outputId": "56317cd3-5b6a-416d-88e5-7f8d82d81200"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "openai_api_key = getpass.getpass(\"Please provide your OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rkAvOadmnjDQ"
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7opJDJLxoEUa"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=index.vectorstore.as_retriever(), return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "2qXK-jOCpQ23",
    "outputId": "b8218565-25ca-4eb0-9320-64ae08cea0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling llama2 API...\n",
      "llama2 API call complete\n",
      "[{'generation': '\\nRetrieval Augmented Generation (RAG) is a novel paradigm for\\ngenerating text by combining a retrieval model with a generative model. The\\nretrieval model is trained to retrieve relevant information from a large corpus\\nof text, and the generative model is trained to generate text based on the\\nretrieved information. This combination of retrieval and generation is often\\nreferred to as \"retrieve-and-generate\" (RAG).\\nRetrieval Augmented Generation is a paradigm for generating text by combining a retrieval model with a generative model. The retrieval model is trained to retrieve relevant information from a large corpus of text, and the generative model is trained to generate text based on the retrieved information. This combination of retrieval and generation is often referred to as \"retrieve-and-generate\" (RAG).\\nRetrieval Augmented Generation is a paradigm for generating text by combining a retrieval model with a generative model. The retrieval model is trained to retrieve relevant information from a large corpus of text, and the generative model is trained to generate text based on the retrieved information. This combination of retrieval'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nRetrieval Augmented Generation (RAG) is a novel paradigm for\\ngenerating text by combining a retrieval model with a generative model. The\\nretrieval model is trained to retrieve relevant information from a large corpus\\nof text, and the generative model is trained to generate text based on the\\nretrieved information. This combination of retrieval and generation is often\\nreferred to as \"retrieve-and-generate\" (RAG).\\nRetrieval Augmented Generation is a paradigm for generating text by combining a retrieval model with a generative model. The retrieval model is trained to retrieve relevant information from a large corpus of text, and the generative model is trained to generate text based on the retrieved information. This combination of retrieval and generation is often referred to as \"retrieve-and-generate\" (RAG).\\nRetrieval Augmented Generation is a paradigm for generating text by combining a retrieval model with a generative model. The retrieval model is trained to retrieve relevant information from a large corpus of text, and the generative model is trained to generate text based on the retrieved information. This combination of retrieval'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Retrieval Augmented Generation?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhrUwVDguy5Q",
    "outputId": "15a87ed5-30a3-46f0-eaf7-5016107377c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling llama2 API...\n",
      "llama2 API call complete\n",
      "[{'generation': ' Retrieval Augmented Generation (RAG) is a new paradigm for building large-scale NLP models that combines the strengths of retrieval and generation. It provides a new way to access and precisely manipulate knowledge, as well as to provide provenance for model decisions.\\n\\n[36] Siva Reddy,\\nDarren Dalton,\\nMohit Iyyer,\\nand Percy Liang.\\nA\\nframework\\nfor\\nfine-grained\\ntext\\ngeneration.\\nIn\\nProceedings\\nof\\nthe\\n2019\\nConference\\non\\nEmpirical\\nMethods\\nin\\nNatural\\nLanguage\\nProcessing,\\npages\\n4831–\\n4842,\\nBrisbane,\\nQueensland,\\nAustralia,\\n2019.\\nURL\\nhttp://papers.nips.cc/paper/\\n9234-a-framework-for-fine-grained-text-generation.pdf.\\n[37]\\nKenton Lee,\\nMohit Iyyer,\\nand Percy Liang.\\nIncorporating\\nre'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Retrieval Augmented Generation?',\n",
       " 'result': ' Retrieval Augmented Generation (RAG) is a new paradigm for building large-scale NLP models that combines the strengths of retrieval and generation. It provides a new way to access and precisely manipulate knowledge, as well as to provide provenance for model decisions.\\n\\n[36] Siva Reddy,\\nDarren Dalton,\\nMohit Iyyer,\\nand Percy Liang.\\nA\\nframework\\nfor\\nfine-grained\\ntext\\ngeneration.\\nIn\\nProceedings\\nof\\nthe\\n2019\\nConference\\non\\nEmpirical\\nMethods\\nin\\nNatural\\nLanguage\\nProcessing,\\npages\\n4831–\\n4842,\\nBrisbane,\\nQueensland,\\nAustralia,\\n2019.\\nURL\\nhttp://papers.nips.cc/paper/\\n9234-a-framework-for-fine-grained-text-generation.pdf.\\n[37]\\nKenton Lee,\\nMohit Iyyer,\\nand Percy Liang.\\nIncorporating\\nre',\n",
       " 'source_documents': [Document(page_content='including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6\\nDiscussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 8, 'producer': 'pdfTeX-1.40.21', 'source': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 7, 'producer': 'pdfTeX-1.40.21', 'source': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto,\\nKelvin Guu,\\nYonatan Oren,\\nand Percy S Liang.\\nA\\nretrieve-and-edit\\nframework\\nfor\\npredicting\\nstructured\\noutputs.\\nIn\\nS.\\nBengio,\\nH. Wallach,\\nH. Larochelle,\\nK. Grauman,\\nN. Cesa-Bianchi,\\nand R. Garnett,\\ned-\\nitors,\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems\\n31,\\npages\\n10052–\\n10062.\\nCurran\\nAssociates,\\nInc.,\\n2018.\\nURL\\nhttp://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 11, 'producer': 'pdfTeX-1.40.21', 'source': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': '/var/folders/wv/tgdcfs0n2095p458yhsw0_2m0000gn/T/tmpd9uqlpgs/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Retrieval Augmented Generation?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
